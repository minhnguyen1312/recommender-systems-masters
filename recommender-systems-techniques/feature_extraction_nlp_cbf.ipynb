{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction using Word2Vec Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import contractions\n",
    "\n",
    "\n",
    "# Print longer cells in pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.options.display.width = 10000\n",
    "# Print all rows\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "from gensim.models import Word2Vec\n",
    "# import gensim.downloader as api\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using Word2Vec Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a custom Word2Vec Model using 2 datasets \n",
    "1. Read data and merge to corpus\n",
    "2. Hypertuning word2vec (Word2Vec is an unsupervised learning algorithm --> no need train/test split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "university_df = pd.read_csv(\"../university_data_normalized.csv\")\n",
    "user_pref_df = pd.read_csv(\"../user_preferences_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_size (int, optional) – Dimensionality of the word vectors.\n",
    "# window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "# min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "# workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "# w2v_model = Word2Vec(documents['tokenized'], vector_size=300, window=8, workers=5, min_count=1)\n",
    "\n",
    "# join data to corpus\n",
    "documents = pd.DataFrame()\n",
    "documents['tokenized'] = pd.concat([university_df['merge_normalize_tokenize'], user_pref_df['normalize_tokenize']])\n",
    "corpus = documents['tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning (uncomment if want to rebuild model)\n",
    "- Run all combination models\n",
    "- Save all model to csv list\n",
    "- Choose best model\n",
    "- Save best model to .model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base model Word2Vec\n",
    "# # base_model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, sg=0, epochs=10)\n",
    "\n",
    "# # Hyperparameter combinations\n",
    "# param_grid = {\n",
    "#     'vector_size': [100, 200, 300],\n",
    "#     'window': [3, 5, 10],\n",
    "#     'min_count': [1, 2],\n",
    "#     'sg': [0, 1],\n",
    "#     'epochs': [15, 25],\n",
    "#     'alpha': [0.025, 0.05]\n",
    "# }\n",
    "# # Generate all combinations of hyperparameters\n",
    "# param_combinations = list(product(\n",
    "#     param_grid['vector_size'],\n",
    "#     param_grid['window'],\n",
    "#     param_grid['min_count'],\n",
    "#     param_grid['sg'],\n",
    "#     param_grid['epochs'],\n",
    "#     param_grid['alpha']\n",
    "# ))\n",
    "\n",
    "# # Store results\n",
    "# results = []\n",
    "# intrinsic_pairs = [('computer','science'),('economics','finance'),('engineering','technology'),('history','sociology'),('psychology','neuroscience'),('philosophy','ethic'),('art','design'),('business','management'),('medicine','healthcare'),('education','teaching')]\n",
    "# for params in param_combinations:\n",
    "#     vector_size, window, min_count, sg, epochs, alpha = params\n",
    "#     print(f\"Training model with: vector_size={vector_size}, window={window}, min_count={min_count}, sg={sg}, epochs={epochs}, alpha={alpha}\")\n",
    "#     model = Word2Vec(corpus, vector_size=vector_size, window=window, min_count=min_count, sg=sg, epochs=epochs, alpha=alpha, workers=5)\n",
    "    \n",
    "#     # Evaluate the model (example: intrinsic evaluation or similarity tasks)\n",
    "#     score = 0\n",
    "#     for a, b in intrinsic_pairs:\n",
    "#         score += model.wv.similarity(a,b)  # Example similarity tasks\n",
    "#     results.append((params, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all hyperparams to csv file\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.columns = ['params', 'computer-science']\n",
    "# with open('Word2Vec_hyperparam.csv', 'a') as file:\n",
    "#     results_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the best parameters\n",
    "# best_params, best_score = max(results, key=lambda x: x[1])\n",
    "# print(f\"Best parameters: {best_params} with score: {best_score}\")\n",
    "\n",
    "# # Retrain with best model\n",
    "# vector_size, window, min_count, sg, epochs, alpha = best_params\n",
    "# # better core can go up to 16 worker\n",
    "# best_model = Word2Vec(corpus, vector_size=vector_size, window=window, min_count=min_count, sg=sg, epochs=epochs, alpha=alpha, workers=5)\n",
    "\n",
    "# # Save the model\n",
    "# best_model.save(\"word2vec_best_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model when needed\n",
    "model = Word2Vec.load(\"word2vec_best_model.model\")\n",
    "\n",
    "# model = Word2Vec(\n",
    "#     corpus,\n",
    "#     vector_size=200, \n",
    "#     window=10, \n",
    "#     min_count=1, \n",
    "#     sg=0, \n",
    "#     epochs=5, \n",
    "#     alpha=0.025, \n",
    "#     workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TF-IDF Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Emphasizes Important Words: Words with higher TF-IDF scores contribute more to the sentence vector.\n",
    "- Reduces Noise: Less important words have less impact on the representation.\\\n",
    "Example: Words like \"computer\" and \"science\" in your example sentence will have higher weights, while words like \"gain\" and \"in\" will have lower influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine all text for TF-IDF computation\n",
    "all_text_contracted = pd.concat([user_pref_df['preference'], university_df['merge_raw']])\n",
    "all_text = all_text_contracted.apply(lambda x: contractions.fix(x))\n",
    "# all_text.to_list()\n",
    "\n",
    "# Init TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Compute\n",
    "tfidf_vectorizer.fit_transform(all_text)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary mapping each word to its IDF weight\n",
    "tfidf_dict = dict(zip(feature_names, tfidf_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Sentence Vectors with TF-IDF Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_weighted_vector(tokens, model, tfidf_dict):\n",
    "    word_vectors = []\n",
    "    total_weight = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in model.wv and word in tfidf_dict:  # Ensure the word is in both Word2Vec and TF-IDF\n",
    "            weight = tfidf_dict[word]\n",
    "            word_vectors.append(model.wv[word] * weight)\n",
    "            total_weight += weight\n",
    "\n",
    "    if word_vectors:  # If there are valid word vectors\n",
    "        return np.sum(word_vectors, axis=0) / total_weight\n",
    "    else:  # Return a zero vector if no valid tokens are found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "documents['sentence_vector'] = documents['tokenized'].apply(lambda x: tfidf_weighted_vector(x, model, tfidf_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Similarities with cosine similarity --- NLP-Based CBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TESTING: try one user with all programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit test_user_id to try other users\n",
    "test_user_id = 41\n",
    "test_user = user_pref_df['normalize_tokenize'].iloc[test_user_id]\n",
    "\n",
    "\n",
    "# Compute TF-IDF weighted vectors for user preference\n",
    "test_user_vector = tfidf_weighted_vector(test_user, model, tfidf_dict)\n",
    "test_user_vector = np.array(test_user_vector).reshape(1, -1) # (1, vector dimension)\n",
    "\n",
    "# Compute TF-IDF weighted vectors for university programs\n",
    "university_df['tfidf_vector'] = university_df['merge_normalize_tokenize'].apply(lambda x: tfidf_weighted_vector(x, model, tfidf_dict))\n",
    "program_vectors = np.stack(university_df['tfidf_vector'].values)\n",
    "\n",
    "# Compute similarity between user1 and all programs\n",
    "similarities = cosine_similarity(test_user_vector, program_vectors)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 41 Preference: I’m excited about a Master of Science in Nanotechnology in Heidelberg, where I can work on cutting-edge materials research.\n",
      "Top 5:                                     program_name                             university  location         degreeType language\n",
      "0          4D-Modern Energy Systems and Mobility  Munich University of Applied Sciences   München             Master   German\n",
      "6949                      Media Culture research                 University of Freiburg  Freiburg     Master of Arts   German\n",
      "6931  Media Computer Science / Media Informatics     Eberhard Karls University Tübingen  Tübingen             Master   German\n",
      "6932                     Media Computer Sciences   Ludwig Maximilians University Munich   München             Master   German\n",
      "6933       Media Computer Science, online course  Berlin University of Applied Sciences    Berlin  Master of Science   German\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df['similarity'] = similarities\n",
    "test_df_sorted = test_df.sort_values(by='similarity', ascending=False)\n",
    "rankingID_top = test_df_sorted.head(5).index.to_list()\n",
    "\n",
    "print(f\"User {test_user_id} Preference: {user_pref_df['preference'].iloc[test_user_id]}\")\n",
    "# print(txt)\n",
    "print(f\"Top 5: {university_df[university_df.columns[[0,2,3,5,6]]].iloc[rankingID_top]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Similarity Scores to Synthetic Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use similarity + introduce noise for synthetic ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_similarity_to_rating(similarities):\n",
    "    # Scale similarity (0-1) to ratings (1-5)\n",
    "    ratings = similarities * 4 + 1  # Scale to 1-5\n",
    "    # Introduce randomness\n",
    "    noise = np.random.normal(0, 0.5, size=similarities.shape)  # Adjust standard deviation as needed\n",
    "    ratings += noise\n",
    "    # Ensure ratings is within bounds (1,5)\n",
    "    ratings = np.clip(ratings, 1, 5)\n",
    "    # Round to nearest half\n",
    "    ratings = np.round(ratings, 2)\n",
    "    return ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_columns = []\n",
    "uni_id = [f'uni_id_{i}' for i in range(similarities.shape[0])]\n",
    "\n",
    "for user_index, user_preference in enumerate(user_pref_df['normalize_tokenize']):\n",
    "    # Compute TF-IDF weighted vectors for user preference\n",
    "    user_vector = tfidf_weighted_vector(user_preference, model, tfidf_dict)\n",
    "    user_vector = np.array(user_vector).reshape(1, -1) # (1, vector dimension)\n",
    "\n",
    "    similarities = cosine_similarity(user_vector, program_vectors)[0]\n",
    "\n",
    "    rating = map_similarity_to_rating(similarities)\n",
    "    user_columns.append(pd.Series(rating, index=uni_id, name=f'userid_{user_index}'))\n",
    "\n",
    "user_ratings = pd.concat(user_columns, axis=1)\n",
    "user_ratings.to_csv('../university_user_ratings.csv',index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
