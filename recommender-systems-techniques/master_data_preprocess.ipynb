{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Print longer cells in pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.options.display.width = 10000\n",
    "# Print all rows\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocessing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ngpbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ngpbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ngpbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ngpbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# ”#$%&'()*+,-./:;?@[\\]^_`{|}~\n",
    "# string.punctuation\n",
    "import string\n",
    "\n",
    "# i'm\n",
    "import contractions\n",
    "\n",
    "# i, am, he, she, on, at\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# for stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# for pos_tag\n",
    "nltk.download('punkt_tab') \n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# english\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append(\"\\'s\")\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for stemming\n",
    "# stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stem instead of lemmanize because simple, interested - interesting similar\n",
    "# lemma instead of stem because of nlp word2vec word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1: Normalize User Preferences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m interested in a Master of Science in Computer Science, ideally in Berlin, where I can gain exposure to cutting-edge AI technologies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Master of Arts in Business Administration with an international focus would align perfectly with my career goals in global management.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’m looking for a Master of Science in Environmental Science in Hamburg, especially one that emphasizes sustainability and climate research.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     preference\n",
       "0      I’m interested in a Master of Science in Computer Science, ideally in Berlin, where I can gain exposure to cutting-edge AI technologies.\n",
       "1      A Master of Arts in Business Administration with an international focus would align perfectly with my career goals in global management.\n",
       "2  I’m looking for a Master of Science in Environmental Science in Hamburg, especially one that emphasizes sustainability and climate research."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load user preference data\n",
    "user_pref_df = pd.read_csv(\"../user_preferences.csv\")\n",
    "user_pref_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to map POS tag to WordNet POS\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  # Adjective\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):  # Verb\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):  # Adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contraction\n",
    "# lowercase\n",
    "# tokenization i am -> ['i','am']\n",
    "# POS_tag\n",
    "# remove punctuation\n",
    "# remove stop words i, will, am, he, a, an, the..., && numbers && Adv\n",
    "# lemmanization\n",
    "\n",
    "def stop_words_removal_then_lemmatize(pos_tags):\n",
    "    processed_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        #check punctuation                          stopwords               digits\n",
    "        if word not in string.punctuation and word not in stopwords and not re.search(r'\\d', word):\n",
    "            # if tag not in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:  # Remove adjectives & adverbs\n",
    "            if tag not in ['RB', 'RBR', 'RBS']:  # Remove adverbs\n",
    "                pos = get_wordnet_pos(tag)\n",
    "                lemma = lemmatizer.lemmatize(word, pos) if pos else lemmatizer.lemmatize(word)\n",
    "                processed_tokens.append(lemma)\n",
    "                # print(f\"{word}, {tag}\")\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "def normalize_tokenize(str):\n",
    "    str = contractions.fix(str)\n",
    "    str = str.lower()\n",
    "\n",
    "    tokens = word_tokenize(str)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    processed_tokens = stop_words_removal_then_lemmatize(pos_tags)\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preference</th>\n",
       "      <th>normalize_tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m interested in a Master of Science in Computer Science, ideally in Berlin, where I can gain exposure to cutting-edge AI technologies.</td>\n",
       "      <td>[interested, master, science, computer, science, berlin, gain, exposure, cutting-edge, ai, technology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Master of Arts in Business Administration with an international focus would align perfectly with my career goals in global management.</td>\n",
       "      <td>[master, art, business, administration, international, focus, would, align, career, goal, global, management]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’m looking for a Master of Science in Environmental Science in Hamburg, especially one that emphasizes sustainability and climate research.</td>\n",
       "      <td>[look, master, science, environmental, science, hamburg, one, emphasize, sustainability, climate, research]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     preference                                                                                             normalize_tokenize\n",
       "0      I’m interested in a Master of Science in Computer Science, ideally in Berlin, where I can gain exposure to cutting-edge AI technologies.         [interested, master, science, computer, science, berlin, gain, exposure, cutting-edge, ai, technology]\n",
       "1      A Master of Arts in Business Administration with an international focus would align perfectly with my career goals in global management.  [master, art, business, administration, international, focus, would, align, career, goal, global, management]\n",
       "2  I’m looking for a Master of Science in Environmental Science in Hamburg, especially one that emphasizes sustainability and climate research.    [look, master, science, environmental, science, hamburg, one, emphasize, sustainability, climate, research]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pref_df['normalize_tokenize'] = user_pref_df['preference'].apply(lambda x: normalize_tokenize(x))\n",
    "user_pref_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2: Normalize Uni Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>program_name</th>\n",
       "      <th>program_url</th>\n",
       "      <th>university</th>\n",
       "      <th>location</th>\n",
       "      <th>duration</th>\n",
       "      <th>degreeType</th>\n",
       "      <th>language</th>\n",
       "      <th>subject</th>\n",
       "      <th>studyMode</th>\n",
       "      <th>admission_Modus</th>\n",
       "      <th>admission_Requirements</th>\n",
       "      <th>overview</th>\n",
       "      <th>teaching</th>\n",
       "      <th>researching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Aesthetics and Media Science</td>\n",
       "      <td>http://www.uni-oldenburg.de/nc/studium/studiengang/?id_studg=312</td>\n",
       "      <td>University of Oldenburg</td>\n",
       "      <td>Oldenburg</td>\n",
       "      <td>4 semesters</td>\n",
       "      <td>Master of Arts</td>\n",
       "      <td>German</td>\n",
       "      <td>Art Studies</td>\n",
       "      <td>full time</td>\n",
       "      <td>without admission restriction</td>\n",
       "      <td>1. Bachelor's degree or equivalent degree in the field of Arts and Media Studies or another subject-specific degree program2. at least 30 credit points for subject-related and didactic content.more information regarding admission requirements. Bachelor/Bakkalaureus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>Economics</td>\n",
       "      <td>https://www.uni-heidelberg.de/de/studium/alle-studienfaecher/economicspolitische-oekonomik/wirtschaftswissenschaft-teilstudiengang-im-master-education</td>\n",
       "      <td>Heidelberg University</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>4 semesters</td>\n",
       "      <td>Master of Education</td>\n",
       "      <td>German</td>\n",
       "      <td>Economic Sciences, Economics</td>\n",
       "      <td>full time</td>\n",
       "      <td>without admission restriction</td>\n",
       "      <td>Admission restrictions, see admission regulations\\;more information regarding admission requirements. Bachelor/Bakkalaureus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>Physics</td>\n",
       "      <td>https://www.zsb.uni-wuppertal.de/studieninfos/studieninfos/master/physik-msc.html</td>\n",
       "      <td>University of Wuppertal</td>\n",
       "      <td>Wuppertal</td>\n",
       "      <td>4 semesters</td>\n",
       "      <td>Master of Science</td>\n",
       "      <td>German</td>\n",
       "      <td>Physics</td>\n",
       "      <td>full time</td>\n",
       "      <td>without admission restriction</td>\n",
       "      <td>Bachelor of Science' or equivalent degree in the Physics course or in a course recognised as equivalent at an institute of higher education in thearea of validity of Germany's Basic Law at least with the grade Satisfactory (3.0) orhas acquired a 'Bachelor of Science' or equivalent degree in the Physics course or in a course recognised as equivalent at an institute of higher education without or outside the area of validity of Germany's Basic Law anda) oral entrance exam lasting 20 to 40 minutes orb) the Graduate Record Examinations Subject (GRE) Test in Physics. Bachelor/Bakkalaureus(and other qualifications, provided that they are recognised as being equivalent)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      program_name                                                                                                                                             program_url               university    location     duration           degreeType language                       subject  studyMode                admission_Modus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           admission_Requirements overview teaching researching\n",
       "100   Aesthetics and Media Science                                                                                        http://www.uni-oldenburg.de/nc/studium/studiengang/?id_studg=312  University of Oldenburg   Oldenburg  4 semesters       Master of Arts   German                   Art Studies  full time  without admission restriction                                                                                                                                                                                                                                                                                                                                                                                                                        1. Bachelor's degree or equivalent degree in the field of Arts and Media Studies or another subject-specific degree program2. at least 30 credit points for subject-related and didactic content.more information regarding admission requirements. Bachelor/Bakkalaureus      NaN      NaN         NaN\n",
       "3000                     Economics  https://www.uni-heidelberg.de/de/studium/alle-studienfaecher/economicspolitische-oekonomik/wirtschaftswissenschaft-teilstudiengang-im-master-education    Heidelberg University  Heidelberg  4 semesters  Master of Education   German  Economic Sciences, Economics  full time  without admission restriction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Admission restrictions, see admission regulations\\;more information regarding admission requirements. Bachelor/Bakkalaureus      NaN      NaN         NaN\n",
       "8000                       Physics                                                                       https://www.zsb.uni-wuppertal.de/studieninfos/studieninfos/master/physik-msc.html  University of Wuppertal   Wuppertal  4 semesters    Master of Science   German                       Physics  full time  without admission restriction  Bachelor of Science' or equivalent degree in the Physics course or in a course recognised as equivalent at an institute of higher education in thearea of validity of Germany's Basic Law at least with the grade Satisfactory (3.0) orhas acquired a 'Bachelor of Science' or equivalent degree in the Physics course or in a course recognised as equivalent at an institute of higher education without or outside the area of validity of Germany's Basic Law anda) oral entrance exam lasting 20 to 40 minutes orb) the Graduate Record Examinations Subject (GRE) Test in Physics. Bachelor/Bakkalaureus(and other qualifications, provided that they are recognised as being equivalent)      NaN      NaN         NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "university_df = pd.read_csv(\"../university_data.csv\")\n",
    "university_df.iloc[[100, 3000, 8000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge: omit url, duration, admission1&2 from merge (no need)\n",
    "university_df['merge_raw'] = university_df[university_df.columns[[0,2,3,5,6,7,8,11,12,13]]].apply(\n",
    "    lambda x: '. '.join(x.dropna().astype(str)), axis=1\n",
    ")\n",
    "university_df['merge_normalize_tokenize'] = university_df['merge_raw'].apply(lambda x: normalize_tokenize(x))\n",
    "university_df['merge_normalize_tokenize'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3: Feature Extraction using Word2Vec Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a custom Word2Vec Model using 2 datasets\n",
    "1. Read data\n",
    "2. Hypertuning word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is an unsupervised learning algorithm --> no need train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "from gensim.models import Word2Vec\n",
    "# import gensim.downloader as api\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_size (int, optional) – Dimensionality of the word vectors.\n",
    "# window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "# min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "# workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "# w2v_model = Word2Vec(documents['tokenized'], vector_size=300, window=8, workers=5, min_count=1)\n",
    "\n",
    "\n",
    "documents = pd.DataFrame()\n",
    "documents['tokenized'] = pd.concat([university_df['merge_normalize_tokenize'], user_pref_df['normalize_tokenize']])\n",
    "corpus = documents['tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model Word2Vec\n",
    "base_model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, sg=0, epochs=10)\n",
    "\n",
    "# Hyperparameter combinations\n",
    "param_grid = {\n",
    "    'vector_size': [100, 200, 300],\n",
    "    'window': [3, 5, 10],\n",
    "    'min_count': [1, 2],\n",
    "    'sg': [0, 1],\n",
    "    'epochs': [15, 25],\n",
    "    'alpha': [0.025, 0.05]\n",
    "}\n",
    "# Generate all combinations of hyperparameters\n",
    "param_combinations = list(product(\n",
    "    param_grid['vector_size'],\n",
    "    param_grid['window'],\n",
    "    param_grid['min_count'],\n",
    "    param_grid['sg'],\n",
    "    param_grid['epochs'],\n",
    "    param_grid['alpha']\n",
    "))\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "intrisic_pairs = [('computer','science'),('economics','finance'),('engineering','technology'),('history','sociology'),('psychology','neuroscience'),('philosophy','ethic'),('art','design'),('business','management'),('medicine','healthcare'),('education','teaching')]\n",
    "for params in param_combinations:\n",
    "    vector_size, window, min_count, sg, epochs, alpha = params\n",
    "    print(f\"Training model with: vector_size={vector_size}, window={window}, min_count={min_count}, sg={sg}, epochs={epochs}, alpha={alpha}\")\n",
    "    model = Word2Vec(corpus, vector_size=vector_size, window=window, min_count=min_count, sg=sg, epochs=epochs, alpha=alpha, workers=5)\n",
    "    \n",
    "    # Evaluate the model (example: intrinsic evaluation or similarity tasks)\n",
    "    similarity_point = 0\n",
    "    for a, b in intrisic_pairs:\n",
    "        similarity_point += model.wv.similarity(a,b)  # Example similarity tasks\n",
    "    results.append((params, similarity_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all hyperparams to csv file\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.columns = ['params', 'similarity_point']\n",
    "with open('Word2Vec_hyperparam.csv', 'a') as file:\n",
    "    results_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best parameters\n",
    "best_params, best_score = max(results, key=lambda x: x[1])\n",
    "print(f\"Best parameters: {best_params} with score: {best_score}\")\n",
    "\n",
    "# Retrain with best model\n",
    "vector_size, window, min_count, sg, epochs, alpha = best_params\n",
    "# better core can go up to 16 worker\n",
    "best_model = Word2Vec(corpus, vector_size=vector_size, window=window, min_count=min_count, sg=sg, epochs=epochs, alpha=alpha, workers=5)\n",
    "\n",
    "# Save the model\n",
    "best_model.save(\"word2vec_best_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model when needed\n",
    "model = Word2Vec.load(\"word2vec_best_model.model\")\n",
    "\n",
    "# model = Word2Vec(\n",
    "#     corpus,\n",
    "#     vector_size=200, \n",
    "#     window=10, \n",
    "#     min_count=1, \n",
    "#     sg=0, \n",
    "#     epochs=5, \n",
    "#     alpha=0.025, \n",
    "#     workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TF-IDF Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Emphasizes Important Words: Words with higher TF-IDF scores contribute more to the sentence vector.\n",
    "- Reduces Noise: Less important words have less impact on the representation.\\\n",
    "Example: Words like \"computer\" and \"science\" in your example sentence will have higher weights, while words like \"gain\" and \"in\" will have lower influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine all text for TF-IDF computation\n",
    "all_text_contracted = pd.concat([user_pref_df['preference'], university_df['merge_raw']])\n",
    "all_text = all_text_contracted.apply(lambda x: contractions.fix(x))\n",
    "# all_text.to_list()\n",
    "\n",
    "# Init TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Compute\n",
    "tfidf_vectorizer.fit_transform(all_text)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary mapping each word to its IDF weight\n",
    "tfidf_dict = dict(zip(feature_names, tfidf_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Sentence Vectors with TF-IDF Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_weighted_vector(tokens, model, tfidf_dict):\n",
    "    word_vectors = []\n",
    "    total_weight = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in model.wv and word in tfidf_dict:  # Ensure the word is in both Word2Vec and TF-IDF\n",
    "            weight = tfidf_dict[word]\n",
    "            word_vectors.append(model.wv[word] * weight)\n",
    "            total_weight += weight\n",
    "\n",
    "    if word_vectors:  # If there are valid word vectors\n",
    "        return np.sum(word_vectors, axis=0) / total_weight\n",
    "    else:  # Return a zero vector if no valid tokens are found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "documents['sentence_vector'] = documents['tokenized'].apply(lambda x: tfidf_weighted_vector(x, model, tfidf_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Similarities with cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TESTING: try one user with all programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_id = 500\n",
    "test_user = user_pref_df['normalize_tokenize'].iloc[test_user_id]\n",
    "\n",
    "# txt = \"An English-taught Master of Science in Renewable Energy Systems in Hamburg would help me focus on solar and wind energy technologies, preparing me for a global career in green energy\"\n",
    "# test_user = normalize_tokenize(txt)\n",
    "\n",
    "# Compute TF-IDF weighted vectors for user preference\n",
    "test_user_vector = tfidf_weighted_vector(test_user, model, tfidf_dict)\n",
    "test_user_vector = np.array(test_user_vector).reshape(1, -1) # (1, vector dimension)\n",
    "\n",
    "# Compute TF-IDF weighted vectors for university programs\n",
    "university_df['tfidf_vector'] = university_df['merge_normalize_tokenize'].apply(lambda x: tfidf_weighted_vector(x, model, tfidf_dict))\n",
    "program_vectors = np.stack(university_df['tfidf_vector'].values)\n",
    "\n",
    "# Compute similarity between user1 and all programs\n",
    "similarities = cosine_similarity(test_user_vector, program_vectors)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 500 Preference: A Master's in blockchain technology appeals to me, especially with applications in finance.\n",
      "Top 5:                                         program_name                                                                      university    location         degreeType language\n",
      "2572                  Data Engineering and Analytics                                                 Munich University of Technology    Garching  Master of Science  English\n",
      "8635           Quantitative Finance and Data Science                                         Berlin School of Applied Sciences (HTW)      Berlin  Master of Science   German\n",
      "6530      Mathematical Finance and Actuarial Science                                                 Munich University of Technology    Garching  Master of Science  English\n",
      "2409  Corporate Sustainability & Sustainable Finance                                          University of Applied Sciences Kempten     Kempten     Master of Arts  English\n",
      "1178              Business Application Architectures  Furtwangen University - computer science, technology, economics, media, health  Furtwangen  Master of Science   German\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df['similarity'] = similarities\n",
    "test_df_sorted = test_df.sort_values(by='similarity', ascending=False)\n",
    "rankingID_top = test_df_sorted.head(5).index.to_list()\n",
    "\n",
    "print(f\"User {test_user_id} Preference: {user_pref_df['preference'].iloc[test_user_id]}\")\n",
    "# print(txt)\n",
    "print(f\"Top 5: {university_df[university_df.columns[[0,2,3,5,6]]].iloc[rankingID_top]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Similarity Scores to Synthetic Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_similarity_to_rating(similarities):\n",
    "    # Scale similarity (0-1) to ratings (1-5)\n",
    "    ratings = similarities * 4 + 1  # Scale to 1-5\n",
    "    # Introduce randomness\n",
    "    noise = np.random.normal(0, 0.5, size=similarities.shape)  # Adjust standard deviation as needed\n",
    "    ratings += noise\n",
    "    # Ensure ratings is within bounds (1,5)\n",
    "    ratings = np.clip(ratings, 1, 5)\n",
    "    # Round to nearest half\n",
    "    ratings = np.round(ratings, 2)\n",
    "    return ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_columns = []\n",
    "uni_id = [f'uni_id_{i}' for i in range(similarities.shape[0])]\n",
    "\n",
    "for user_index, user_preference in enumerate(user_pref_df['normalize_tokenize']):\n",
    "    # Compute TF-IDF weighted vectors for user preference\n",
    "    user_vector = tfidf_weighted_vector(user_preference, model, tfidf_dict)\n",
    "    user_vector = np.array(user_vector).reshape(1, -1) # (1, vector dimension)\n",
    "\n",
    "    similarities = cosine_similarity(user_vector, program_vectors)[0]\n",
    "\n",
    "    rating = map_similarity_to_rating(similarities)\n",
    "    user_columns.append(pd.Series(rating, index=uni_id, name=f'userid_{user_index}'))\n",
    "\n",
    "user_ratings = pd.concat(user_columns, axis=1)\n",
    "user_ratings.to_csv('../university_user_ratings.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4: Introduce Data Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate real-world data where users rate only a subset of items, have each user rate a random selection of programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = pd.read_csv('../university_user_ratings.csv').set_index('Unnamed: 0')\n",
    "user_ratings.index.names = ['index']\n",
    "\n",
    "data_sparse_user_ratings = pd.DataFrame().reindex_like(user_ratings)\n",
    "\n",
    "for user_index in range(user_ratings.shape[1]):\n",
    "    # Each user randomly rate K (10-30) items\n",
    "    k = random.randint(500,2000)\n",
    "    uni_id_to_rate = random.sample(list(range(user_ratings.shape[0])), k=k) # choose which uni_ids will be evaluated and put in list\n",
    "    for uni_id in uni_id_to_rate:\n",
    "        data_sparse_user_ratings.loc[f'uni_id_{uni_id}', f'userid_{user_index}'] \\\n",
    "            = user_ratings.loc[f'uni_id_{uni_id}', f'userid_{user_index}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ratings = pd.read_csv('../university_user_ratings.csv').set_index('Unnamed: 0')\n",
    "# user_ratings.index.names = ['index']\n",
    "# data_sparse_user_ratings = user_ratings.copy()\n",
    "\n",
    "# for user_index in range(user_ratings.shape[1]):\n",
    "#     # Each user randomly rate K (10-30) items\n",
    "#     k = random.randint(100,200)\n",
    "#     uni_id_to_rate = random.sample(list(range(user_ratings.shape[0])), k=k) # choose which uni_ids will be evaluated and put in list\n",
    "#     for uni_id in uni_id_to_rate:\n",
    "#         data_sparse_user_ratings.loc[f'uni_id_{uni_id}', f'userid_{user_index}'] \\\n",
    "#             = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.8800\n",
      "Density: 0.1200\n",
      "User-Item Matrix:\n",
      "user_id      userid_0  userid_1  userid_10  userid_100  userid_101  \\\n",
      "uni_id                                                               \n",
      "uni_id_0          NaN       NaN        NaN         NaN         NaN   \n",
      "uni_id_1          NaN       NaN        NaN         NaN         NaN   \n",
      "uni_id_10         NaN       NaN        NaN         NaN         NaN   \n",
      "uni_id_100        NaN       4.0        NaN         NaN         NaN   \n",
      "uni_id_1000       3.5       NaN        NaN         2.5         NaN   \n",
      "\n",
      "user_id      userid_102  userid_103  userid_104  userid_105  userid_106  ...  \\\n",
      "uni_id                                                                   ...   \n",
      "uni_id_0            NaN         NaN         3.5         NaN         5.0  ...   \n",
      "uni_id_1            NaN         NaN         NaN         NaN         3.5  ...   \n",
      "uni_id_10           NaN         NaN         NaN         NaN         NaN  ...   \n",
      "uni_id_100          NaN         NaN         NaN         3.5         NaN  ...   \n",
      "uni_id_1000         NaN         NaN         NaN         NaN         NaN  ...   \n",
      "\n",
      "user_id      userid_90  userid_91  userid_92  userid_93  userid_94  userid_95  \\\n",
      "uni_id                                                                          \n",
      "uni_id_0           NaN        NaN        2.5        NaN        NaN        NaN   \n",
      "uni_id_1           NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "uni_id_10          NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "uni_id_100         NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "uni_id_1000        NaN        NaN        NaN        NaN        NaN        NaN   \n",
      "\n",
      "user_id      userid_96  userid_97  userid_98  userid_99  \n",
      "uni_id                                                   \n",
      "uni_id_0           NaN        NaN        3.0        NaN  \n",
      "uni_id_1           NaN        NaN        NaN        NaN  \n",
      "uni_id_10          NaN        NaN        4.0        NaN  \n",
      "uni_id_100         NaN        NaN        NaN        NaN  \n",
      "uni_id_1000        3.5        NaN        NaN        NaN  \n",
      "\n",
      "[5 rows x 554 columns]\n"
     ]
    }
   ],
   "source": [
    "melted_df_ratings = data_sparse_user_ratings.reset_index().melt(id_vars=['index'], var_name='user_id', value_name='ratings')\n",
    "melted_df_ratings.rename(columns={'index': 'uni_id'}, inplace=True)\n",
    "melted_df_ratings.to_csv('../user_item_df.csv', index=False)\n",
    "\n",
    "# calculate density & sparsity\n",
    "tmp = melted_df_ratings.count()\n",
    "actual_ratings = tmp['ratings']\n",
    "total_possible_entries = tmp['user_id']\n",
    "sparsity = 1 - (actual_ratings / total_possible_entries)\n",
    "density = actual_ratings / total_possible_entries\n",
    "print(f\"Sparsity: {sparsity:.4f}\")\n",
    "print(f\"Density: {density:.4f}\")\n",
    "\n",
    "ratings_matrix = melted_df_ratings.pivot(index='uni_id', columns='user_id', values='ratings')\n",
    "# print(ratings_matrix)\n",
    "print('User-Item Matrix:')\n",
    "print(ratings_matrix.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise is a Python scikit for building and analyzing recommender systems that deal with explicit rating data.\n",
    "# $ conda install -c conda-forge scikit-surprise\n",
    "from surprise import KNNBasic\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD, NMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../user_item_df.csv').dropna()\n",
    "\n",
    "reader = Reader() #default is already 1-5\n",
    "dataset = Dataset.load_from_df(data[['user_id','uni_id','ratings']], reader) #It must have three columns, corresponding to the user (raw) ids, the item (raw) ids, and the ratings, in this order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uni_id     5763816\n",
       "user_id    5763816\n",
       "ratings    5681007\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-based CF using kNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.5752  0.5741  0.5751  0.5748  0.0005  \n",
      "MAE (testset)     0.4589  0.4581  0.4592  0.4587  0.0005  \n",
      "Fit time          1.77    1.97    2.12    1.96    0.14    \n",
      "Test time         17.75   17.94   17.05   17.58   0.38    \n"
     ]
    }
   ],
   "source": [
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity\n",
    "    'user_based': True,  # User-based collaborative filtering\n",
    "    'min_support': 3,   # Minimum number of common items for similarity\n",
    "    'shrinkage': 100    # Shrinkage parameter in case of sparse data\n",
    "}\n",
    "\n",
    "# Define the algorithm\n",
    "user_cf = KNNBasic(k=20, min_k=1,sim_options=sim_options,verbose=True)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "user_cf_cv_results = cross_validate(user_cf, dataset, measures=['RMSE', 'MAE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.5909  0.5917  0.5911  0.5912  0.0003  \n",
      "MAE (testset)     0.4714  0.4716  0.4718  0.4716  0.0002  \n",
      "Fit time          75.10   65.70   65.99   68.93   4.36    \n",
      "Test time         159.62  173.46  171.87  168.32  6.19    \n"
     ]
    }
   ],
   "source": [
    "# Define Item-Based CF algorithm\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity\n",
    "    'user_based': False,  # Item-based collaborative filtering\n",
    "    'min_support': 5,   # Minimum number of common items for similarity\n",
    "    'shrinkage': 100    # Shrinkage parameter in case of sparse data\n",
    "}\n",
    "item_cf = KNNBasic(k=20, min_k=1,sim_options=sim_options, verbose=True)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "item_cf_cv_results = cross_validate(item_cf, dataset, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Vector Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Evaluating RMSE, MAE of algorithm SVD on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.5520  0.5524  0.5536  0.5527  0.0007  \n",
      "MAE (testset)     0.4407  0.4414  0.4419  0.4413  0.0005  \n",
      "Fit time          3.57    3.67    3.77    3.67    0.08    \n",
      "Test time         2.14    2.47    2.18    2.26    0.15    \n"
     ]
    }
   ],
   "source": [
    "# Define SVD algorithm\n",
    "svd_algo = SVD(n_factors=10, n_epochs=20,verbose=True)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "svd_cv_results = cross_validate(svd_algo, dataset, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm NMF on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.6708  0.6726  0.6770  0.6735  0.0026  \n",
      "MAE (testset)     0.5397  0.5411  0.5444  0.5417  0.0019  \n",
      "Fit time          3.90    3.93    3.80    3.88    0.06    \n",
      "Test time         2.16    2.54    2.10    2.27    0.20    \n"
     ]
    }
   ],
   "source": [
    "# Define NMF algorithm\n",
    "nmf_algo = NMF(n_factors=10, n_epochs=20,biased=False)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "nmf_cv_results = cross_validate(nmf_algo, dataset, measures=['RMSE', 'MAE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Based CF - RMSE: 0.5748 (± 0.0005), MAE: 0.4587 (± 0.0005)\n",
      "Item-Based CF - RMSE: 0.5912 (± 0.0003), MAE: 0.4716 (± 0.0002)\n",
      "SVD - RMSE: 0.5527 (± 0.0007), MAE: 0.4413 (± 0.0005)\n",
      "NMF - RMSE: 0.6735 (± 0.0026), MAE: 0.5417 (± 0.0019)\n"
     ]
    }
   ],
   "source": [
    "def print_cv_results(algo_name, cv_results):\n",
    "    mean_rmse = cv_results['test_rmse'].mean()\n",
    "    mean_mae = cv_results['test_mae'].mean()\n",
    "    std_rmse = cv_results['test_rmse'].std()\n",
    "    std_mae = cv_results['test_mae'].std()\n",
    "    print(f\"{algo_name} - RMSE: {mean_rmse:.4f} (± {std_rmse:.4f}), MAE: {mean_mae:.4f} (± {std_mae:.4f})\")\n",
    "\n",
    "print_cv_results(\"User-Based CF\", user_cf_cv_results)\n",
    "print_cv_results(\"Item-Based CF\", item_cf_cv_results)\n",
    "print_cv_results(\"SVD\", svd_cv_results)\n",
    "print_cv_results(\"NMF\", nmf_cv_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
