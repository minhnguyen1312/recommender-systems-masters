{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Print longer cells in pd\n",
    "pd.options.display.max_colwidth = 1000\n",
    "pd.options.display.width = 10000\n",
    "# Print all rows\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocessing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ngpbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ngpbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ngpbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\ngpbm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# ”#$%&'()*+,-./:;?@[\\]^_`{|}~\n",
    "# string.punctuation\n",
    "import string\n",
    "\n",
    "# i'm\n",
    "import contractions\n",
    "\n",
    "# i, am, he, she, on, at\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# for stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# for pos_tag\n",
    "nltk.download('punkt_tab') \n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# english\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append(\"\\'s\")\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#defining the object for stemming\n",
    "# stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# stem instead of lemmanize because simple, interested - interesting similar\n",
    "# lemma instead of stem because of nlp word2vec word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S1: Normalize User Preferences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m interested in a Master of Science in Computer Science, ideally in Berlin, where I can gain exposure to cutting-edge AI technologies.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Master of Arts in Business Administration with an international focus would align perfectly with my career goals in global management.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’m looking for a Master of Science in Environmental Science in Hamburg, especially one that emphasizes sustainability and climate research.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     preference\n",
       "0      I’m interested in a Master of Science in Computer Science, ideally in Berlin, where I can gain exposure to cutting-edge AI technologies.\n",
       "1      A Master of Arts in Business Administration with an international focus would align perfectly with my career goals in global management.\n",
       "2  I’m looking for a Master of Science in Environmental Science in Hamburg, especially one that emphasizes sustainability and climate research."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load user preference data\n",
    "user_pref_df = pd.read_csv(\"../user_preferences.csv\")\n",
    "user_pref_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to map POS tag to WordNet POS\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  # Adjective\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):  # Verb\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):  # Adverb\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contraction\n",
    "# lowercase\n",
    "# tokenization i am -> ['i','am']\n",
    "# POS_tag\n",
    "# remove punctuation\n",
    "# remove stop words i, will, am, he, a, an, the..., && numbers && Adv\n",
    "# lemmanization\n",
    "\n",
    "def stop_words_removal_then_lemmatize(pos_tags):\n",
    "    processed_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        #check punctuation                          stopwords               digits\n",
    "        if word not in string.punctuation and word not in stopwords and not re.search(r'\\d', word):\n",
    "            # if tag not in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:  # Remove adjectives & adverbs\n",
    "            if tag not in ['RB', 'RBR', 'RBS']:  # Remove adverbs\n",
    "                pos = get_wordnet_pos(tag)\n",
    "                lemma = lemmatizer.lemmatize(word, pos) if pos else lemmatizer.lemmatize(word)\n",
    "                processed_tokens.append(lemma)\n",
    "                # print(f\"{word}, {tag}\")\n",
    "    return processed_tokens\n",
    "\n",
    "\n",
    "def normalize_tokenize(str):\n",
    "    str = contractions.fix(str)\n",
    "    str = str.lower()\n",
    "\n",
    "    tokens = word_tokenize(str)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "\n",
    "    processed_tokens = stop_words_removal_then_lemmatize(pos_tags)\n",
    "    \n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preference</th>\n",
       "      <th>normalize_tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m interested in a Master of Science in Computer Science, ideally in Berlin, where I can gain exposure to cutting-edge AI technologies.</td>\n",
       "      <td>[interested, master, science, computer, science, berlin, gain, exposure, cutting-edge, ai, technology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A Master of Arts in Business Administration with an international focus would align perfectly with my career goals in global management.</td>\n",
       "      <td>[master, art, business, administration, international, focus, would, align, career, goal, global, management]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’m looking for a Master of Science in Environmental Science in Hamburg, especially one that emphasizes sustainability and climate research.</td>\n",
       "      <td>[look, master, science, environmental, science, hamburg, one, emphasize, sustainability, climate, research]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     preference                                                                                             normalize_tokenize\n",
       "0      I’m interested in a Master of Science in Computer Science, ideally in Berlin, where I can gain exposure to cutting-edge AI technologies.         [interested, master, science, computer, science, berlin, gain, exposure, cutting-edge, ai, technology]\n",
       "1      A Master of Arts in Business Administration with an international focus would align perfectly with my career goals in global management.  [master, art, business, administration, international, focus, would, align, career, goal, global, management]\n",
       "2  I’m looking for a Master of Science in Environmental Science in Hamburg, especially one that emphasizes sustainability and climate research.    [look, master, science, environmental, science, hamburg, one, emphasize, sustainability, climate, research]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pref_df['normalize_tokenize'] = user_pref_df['preference'].apply(lambda x: normalize_tokenize(x))\n",
    "user_pref_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2: Normalize Uni Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>program_name</th>\n",
       "      <th>program_url</th>\n",
       "      <th>university</th>\n",
       "      <th>location</th>\n",
       "      <th>duration</th>\n",
       "      <th>degreeType</th>\n",
       "      <th>language</th>\n",
       "      <th>subject</th>\n",
       "      <th>studyMode</th>\n",
       "      <th>admission_Modus</th>\n",
       "      <th>admission_Requirements</th>\n",
       "      <th>overview</th>\n",
       "      <th>teaching</th>\n",
       "      <th>researching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Aesthetics and Media Science</td>\n",
       "      <td>http://www.uni-oldenburg.de/nc/studium/studiengang/?id_studg=312</td>\n",
       "      <td>University of Oldenburg</td>\n",
       "      <td>Oldenburg</td>\n",
       "      <td>4 semesters</td>\n",
       "      <td>Master of Arts</td>\n",
       "      <td>German</td>\n",
       "      <td>Art Studies</td>\n",
       "      <td>full time</td>\n",
       "      <td>without admission restriction</td>\n",
       "      <td>1. Bachelor's degree or equivalent degree in the field of Arts and Media Studies or another subject-specific degree program2. at least 30 credit points for subject-related and didactic content.more information regarding admission requirements. Bachelor/Bakkalaureus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>Economics</td>\n",
       "      <td>https://www.uni-heidelberg.de/de/studium/alle-studienfaecher/economicspolitische-oekonomik/wirtschaftswissenschaft-teilstudiengang-im-master-education</td>\n",
       "      <td>Heidelberg University</td>\n",
       "      <td>Heidelberg</td>\n",
       "      <td>4 semesters</td>\n",
       "      <td>Master of Education</td>\n",
       "      <td>German</td>\n",
       "      <td>Economic Sciences, Economics</td>\n",
       "      <td>full time</td>\n",
       "      <td>without admission restriction</td>\n",
       "      <td>Admission restrictions, see admission regulations\\;more information regarding admission requirements. Bachelor/Bakkalaureus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>Physics</td>\n",
       "      <td>https://www.zsb.uni-wuppertal.de/studieninfos/studieninfos/master/physik-msc.html</td>\n",
       "      <td>University of Wuppertal</td>\n",
       "      <td>Wuppertal</td>\n",
       "      <td>4 semesters</td>\n",
       "      <td>Master of Science</td>\n",
       "      <td>German</td>\n",
       "      <td>Physics</td>\n",
       "      <td>full time</td>\n",
       "      <td>without admission restriction</td>\n",
       "      <td>Bachelor of Science' or equivalent degree in the Physics course or in a course recognised as equivalent at an institute of higher education in thearea of validity of Germany's Basic Law at least with the grade Satisfactory (3.0) orhas acquired a 'Bachelor of Science' or equivalent degree in the Physics course or in a course recognised as equivalent at an institute of higher education without or outside the area of validity of Germany's Basic Law anda) oral entrance exam lasting 20 to 40 minutes orb) the Graduate Record Examinations Subject (GRE) Test in Physics. Bachelor/Bakkalaureus(and other qualifications, provided that they are recognised as being equivalent)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      program_name                                                                                                                                             program_url               university    location     duration           degreeType language                       subject  studyMode                admission_Modus                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           admission_Requirements overview teaching researching\n",
       "100   Aesthetics and Media Science                                                                                        http://www.uni-oldenburg.de/nc/studium/studiengang/?id_studg=312  University of Oldenburg   Oldenburg  4 semesters       Master of Arts   German                   Art Studies  full time  without admission restriction                                                                                                                                                                                                                                                                                                                                                                                                                        1. Bachelor's degree or equivalent degree in the field of Arts and Media Studies or another subject-specific degree program2. at least 30 credit points for subject-related and didactic content.more information regarding admission requirements. Bachelor/Bakkalaureus      NaN      NaN         NaN\n",
       "3000                     Economics  https://www.uni-heidelberg.de/de/studium/alle-studienfaecher/economicspolitische-oekonomik/wirtschaftswissenschaft-teilstudiengang-im-master-education    Heidelberg University  Heidelberg  4 semesters  Master of Education   German  Economic Sciences, Economics  full time  without admission restriction                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Admission restrictions, see admission regulations\\;more information regarding admission requirements. Bachelor/Bakkalaureus      NaN      NaN         NaN\n",
       "8000                       Physics                                                                       https://www.zsb.uni-wuppertal.de/studieninfos/studieninfos/master/physik-msc.html  University of Wuppertal   Wuppertal  4 semesters    Master of Science   German                       Physics  full time  without admission restriction  Bachelor of Science' or equivalent degree in the Physics course or in a course recognised as equivalent at an institute of higher education in thearea of validity of Germany's Basic Law at least with the grade Satisfactory (3.0) orhas acquired a 'Bachelor of Science' or equivalent degree in the Physics course or in a course recognised as equivalent at an institute of higher education without or outside the area of validity of Germany's Basic Law anda) oral entrance exam lasting 20 to 40 minutes orb) the Graduate Record Examinations Subject (GRE) Test in Physics. Bachelor/Bakkalaureus(and other qualifications, provided that they are recognised as being equivalent)      NaN      NaN         NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "university_df = pd.read_csv(\"../university_data.csv\")\n",
    "university_df.iloc[[100, 3000, 8000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [energy, system, mobility, munich, university, applied, science, münchen, master, german, electrical, engineering, part, time, part, time, degree, programme, professional]\n",
       "1                                                                  [accordion, munich, college, music, theatre, münchen, master, music, german, instrumental, music, full, time]\n",
       "2                                                                      [accordion, college, music, nürnberg/augsburg, nürnberg, master, german, instrumental, music, full, time]\n",
       "Name: merge_normalize_tokenize, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge: omit url, duration, admission1&2 from merge (no need)\n",
    "university_df['merge_raw'] = university_df[university_df.columns[[0,2,3,5,6,7,8,11,12,13]]].apply(\n",
    "    lambda x: '. '.join(x.dropna().astype(str)), axis=1\n",
    ")\n",
    "university_df['merge_normalize_tokenize'] = university_df['merge_raw'].apply(lambda x: normalize_tokenize(x))\n",
    "university_df['merge_normalize_tokenize'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3: Feature Extraction using Word2Vec Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a custom Word2Vec Model using 2 datasets\n",
    "1. Read data\n",
    "2. Hypertuning word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is an unsupervised learning algorithm --> no need train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "from gensim.models import Word2Vec\n",
    "# import gensim.downloader as api\n",
    "from itertools import product\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_size (int, optional) – Dimensionality of the word vectors.\n",
    "# window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "# min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "# workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "# w2v_model = Word2Vec(documents['tokenized'], vector_size=300, window=8, workers=5, min_count=1)\n",
    "\n",
    "\n",
    "documents = pd.DataFrame()\n",
    "documents['tokenized'] = pd.concat([university_df['merge_normalize_tokenize'], user_pref_df['normalize_tokenize']])\n",
    "corpus = documents['tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Base model Word2Vec\n",
    "# base_model = Word2Vec(corpus, vector_size=100, window=5, min_count=1, sg=0, epochs=10)\n",
    "\n",
    "# # Hyperparameter combinations\n",
    "# param_grid = {\n",
    "#     'vector_size': [100, 200, 300],\n",
    "#     'window': [3, 5, 10],\n",
    "#     'min_count': [1, 2],\n",
    "#     'sg': [0, 1],\n",
    "#     'epochs': [15, 25],\n",
    "#     'alpha': [0.025, 0.05]\n",
    "# }\n",
    "# # Generate all combinations of hyperparameters\n",
    "# param_combinations = list(product(\n",
    "#     param_grid['vector_size'],\n",
    "#     param_grid['window'],\n",
    "#     param_grid['min_count'],\n",
    "#     param_grid['sg'],\n",
    "#     param_grid['epochs'],\n",
    "#     param_grid['alpha']\n",
    "# ))\n",
    "\n",
    "# # Store results\n",
    "# results = []\n",
    "# intrisic_pairs = [('computer','science'),('economics','finance'),('engineering','technology'),('history','sociology'),('psychology','neuroscience'),('philosophy','ethic'),('art','design'),('business','management'),('medicine','healthcare'),('education','teaching')]\n",
    "# for params in param_combinations:\n",
    "#     vector_size, window, min_count, sg, epochs, alpha = params\n",
    "#     print(f\"Training model with: vector_size={vector_size}, window={window}, min_count={min_count}, sg={sg}, epochs={epochs}, alpha={alpha}\")\n",
    "#     model = Word2Vec(corpus, vector_size=vector_size, window=window, min_count=min_count, sg=sg, epochs=epochs, alpha=alpha, workers=5)\n",
    "    \n",
    "#     # Evaluate the model (example: intrinsic evaluation or similarity tasks)\n",
    "#     similarity_point = 0\n",
    "#     for a, b in intrisic_pairs:\n",
    "#         similarity_point += model.wv.similarity(a,b)  # Example similarity tasks\n",
    "#     results.append((params, similarity_point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save all hyperparams to csv file\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.columns = ['params', 'similarity_point']\n",
    "# with open('Word2Vec_hyperparam.csv', 'a') as file:\n",
    "#     results_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the best parameters\n",
    "# best_params, best_score = max(results, key=lambda x: x[1])\n",
    "# print(f\"Best parameters: {best_params} with score: {best_score}\")\n",
    "\n",
    "# # Retrain with best model\n",
    "# vector_size, window, min_count, sg, epochs, alpha = best_params\n",
    "# # better core can go up to 16 worker\n",
    "# best_model = Word2Vec(corpus, vector_size=vector_size, window=window, min_count=min_count, sg=sg, epochs=epochs, alpha=alpha, workers=5)\n",
    "\n",
    "# # Save the model\n",
    "# best_model.save(\"word2vec_best_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model when needed\n",
    "# model = Word2Vec.load(\"word2vec_best_model.model\")\n",
    "\n",
    "model = Word2Vec(\n",
    "    corpus,\n",
    "    vector_size=200, \n",
    "    window=10, \n",
    "    min_count=1, \n",
    "    sg=0, \n",
    "    epochs=5, \n",
    "    alpha=0.025, \n",
    "    workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TF-IDF Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Emphasizes Important Words: Words with higher TF-IDF scores contribute more to the sentence vector.\n",
    "- Reduces Noise: Less important words have less impact on the representation.\\\n",
    "Example: Words like \"computer\" and \"science\" in your example sentence will have higher weights, while words like \"gain\" and \"in\" will have lower influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine all text for TF-IDF computation\n",
    "all_text_contracted = pd.concat([user_pref_df['preference'], university_df['merge_raw']])\n",
    "all_text = all_text_contracted.apply(lambda x: contractions.fix(x))\n",
    "# all_text.to_list()\n",
    "\n",
    "# Init TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Compute\n",
    "tfidf_vectorizer.fit_transform(all_text)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary mapping each word to its IDF weight\n",
    "tfidf_dict = dict(zip(feature_names, tfidf_vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Sentence Vectors with TF-IDF Weighted Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_weighted_vector(tokens, model, tfidf_dict):\n",
    "    word_vectors = []\n",
    "    total_weight = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv and word in tfidf_dict:  # Ensure the word is in both Word2Vec and TF-IDF\n",
    "            weight = tfidf_dict[word]\n",
    "            word_vectors.append(model.wv[word] * weight)\n",
    "            total_weight += weight\n",
    "\n",
    "    if word_vectors:  # If there are valid word vectors\n",
    "        return np.sum(word_vectors, axis=0) / total_weight\n",
    "    else:  # Return a zero vector if no valid tokens are found\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "documents['sentence_vector'] = documents['tokenized'].apply(lambda x: tfidf_weighted_vector(x, model, tfidf_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenized          object\n",
       "sentence_vector    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Similarities with cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TESTING: try one user with all programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_id = 420\n",
    "test_user = user_pref_df['normalize_tokenize'].iloc[test_user_id]\n",
    "\n",
    "\n",
    "\n",
    "# Compute TF-IDF weighted vectors for user preference\n",
    "test_user_vector = tfidf_weighted_vector(test_user, model, tfidf_dict)\n",
    "test_user_vector = np.array(test_user_vector).reshape(1, -1) # (1, vector dimension)\n",
    "\n",
    "# Compute TF-IDF weighted vectors for university programs\n",
    "university_df['tfidf_vector'] = university_df['merge_normalize_tokenize'].apply(lambda x: tfidf_weighted_vector(x, model, tfidf_dict))\n",
    "program_vectors = np.stack(university_df['tfidf_vector'].values)\n",
    "\n",
    "# Compute similarity between user1 and all programs\n",
    "similarities = cosine_similarity(test_user_vector, program_vectors)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 420 Preference: A Master's in biochemistry appeals to me, especially with a focus on medical applications.\n",
      "Top 5:                                                                     program_name                                         university    location                            degreeType language\n",
      "7088                                                           Medicinal Physics                     University of Halle-Wittenberg       Halle  Master of Science, 120 credit points   German\n",
      "8794                                                    Renewable Energy Sources                     University of Halle-Wittenberg       Halle  Master of Science, 120 credit points   German\n",
      "9799  Sustainable Regional Development: Education-Management-Nature Conservation  Eberswalde University for Sustainable Development  Eberswalde                     Master of Science   German\n",
      "7045                                                         Medical Informatics                             University of Augsburg    Augsburg                                Master   German\n",
      "9813                                               Sustainable Urban Development              Anhalt University of Applied Sciences      Dessau                                Master   German\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df['similarity'] = similarities\n",
    "test_df_sorted = test_df.sort_values(by='similarity', ascending=False)\n",
    "rankingID_top = test_df_sorted.head(5).index.to_list()\n",
    "\n",
    "print(f\"User {test_user_id} Preference: {user_pref_df['preference'].iloc[test_user_id]}\")\n",
    "# print(txt)\n",
    "print(f\"Top 5: {university_df[university_df.columns[[0,2,3,5,6]]].iloc[rankingID_top]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Similarity Scores to Synthetic Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_similarity_to_rating(similarities):\n",
    "    # Scale similarity (0-1) to ratings (1-5)\n",
    "    ratings = similarities * 4 + 1  # Scale to 1-5\n",
    "    # Introduce randomness\n",
    "    noise = np.random.normal(0, 0.5, size=similarities.shape)  # Adjust standard deviation as needed\n",
    "    ratings += noise\n",
    "    # Ensure ratings is within bounds (1,5)\n",
    "    ratings = np.clip(ratings, 1, 5)\n",
    "    # Round to nearest half\n",
    "    ratings = np.round(ratings, 2)\n",
    "    return ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_columns = []\n",
    "uni_id = [f'uni_id_{i}' for i in range(similarities.shape[0])]\n",
    "\n",
    "for user_index, user_preference in enumerate(user_pref_df['normalize_tokenize']):\n",
    "    # Compute TF-IDF weighted vectors for user preference\n",
    "    user_vector = tfidf_weighted_vector(user_preference, model, tfidf_dict)\n",
    "    user_vector = np.array(user_vector).reshape(1, -1) # (1, vector dimension)\n",
    "\n",
    "    similarities = cosine_similarity(user_vector, program_vectors)[0]\n",
    "\n",
    "    rating = map_similarity_to_rating(similarities)\n",
    "    user_columns.append(pd.Series(rating, index=uni_id, name=f'userid_{user_index}'))\n",
    "\n",
    "user_ratings = pd.concat(user_columns, axis=1)\n",
    "user_ratings.to_csv('../university_user_ratings.csv',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S4: Introduce Data Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate real-world data where users rate only a subset of items, have each user rate a random selection of programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = pd.read_csv('../university_user_ratings.csv').set_index('Unnamed: 0')\n",
    "user_ratings.index.names = ['index']\n",
    "\n",
    "data_sparse_user_ratings = pd.DataFrame().reindex_like(user_ratings)\n",
    "\n",
    "for user_index in range(user_ratings.shape[1]):\n",
    "    # Each user randomly rate K (10-30) items\n",
    "    k = random.randint(500,2000)\n",
    "    uni_id_to_rate = random.sample(list(range(user_ratings.shape[0])), k=k) # choose which uni_ids will be evaluated and put in list\n",
    "    for uni_id in uni_id_to_rate:\n",
    "        data_sparse_user_ratings.loc[f'uni_id_{uni_id}', f'userid_{user_index}'] \\\n",
    "            = user_ratings.loc[f'uni_id_{uni_id}', f'userid_{user_index}']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ratings = pd.read_csv('../university_user_ratings.csv').set_index('Unnamed: 0')\n",
    "# user_ratings.index.names = ['index']\n",
    "# data_sparse_user_ratings = user_ratings.copy()\n",
    "\n",
    "# for user_index in range(user_ratings.shape[1]):\n",
    "#     # Each user randomly rate K (10-30) items\n",
    "#     k = random.randint(100,200)\n",
    "#     uni_id_to_rate = random.sample(list(range(user_ratings.shape[0])), k=k) # choose which uni_ids will be evaluated and put in list\n",
    "#     for uni_id in uni_id_to_rate:\n",
    "#         data_sparse_user_ratings.loc[f'uni_id_{uni_id}', f'userid_{user_index}'] \\\n",
    "#             = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.8819\n",
      "Density: 0.1181\n",
      "User-Item Matrix:\n",
      "user_id      userid_0  userid_1  userid_10  userid_100  userid_101  userid_102  userid_103  userid_104  userid_105  userid_106  userid_107  userid_108  userid_109  userid_11  userid_110  userid_111  userid_112  userid_113  userid_114  userid_115  userid_116  userid_117  userid_118  userid_119  userid_12  userid_120  userid_121  userid_122  userid_123  userid_124  userid_125  userid_126  userid_127  userid_128  userid_129  userid_13  userid_130  userid_131  userid_132  userid_133  userid_134  userid_135  userid_136  userid_137  userid_138  userid_139  userid_14  userid_140  userid_141  userid_142  userid_143  userid_144  userid_145  userid_146  userid_147  userid_148  userid_149  userid_15  userid_150  userid_151  userid_152  userid_153  userid_154  userid_155  userid_156  userid_157  userid_158  userid_159  userid_16  userid_160  userid_161  userid_162  userid_163  userid_164  userid_165  userid_166  userid_167  userid_168  userid_169  userid_17  userid_170  userid_171  userid_172  userid_173  userid_174  userid_175  userid_176  userid_177  userid_178  userid_179  userid_18  userid_180  userid_181  userid_182  userid_183  userid_184  userid_185  userid_186  userid_187  userid_188  userid_189  userid_19  userid_190  userid_191  userid_192  userid_193  userid_194  userid_195  userid_196  userid_197  userid_198  userid_199  userid_2  userid_20  userid_200  userid_201  userid_202  userid_203  userid_204  userid_205  userid_206  userid_207  userid_208  userid_209  userid_21  userid_210  userid_211  userid_212  userid_213  userid_214  userid_215  userid_216  userid_217  userid_218  userid_219  userid_22  userid_220  userid_221  userid_222  userid_223  userid_224  userid_225  userid_226  userid_227  userid_228  userid_229  userid_23  userid_230  userid_231  userid_232  userid_233  userid_234  userid_235  userid_236  userid_237  userid_238  userid_239  userid_24  userid_240  userid_241  userid_242  userid_243  userid_244  userid_245  userid_246  userid_247  userid_248  userid_249  userid_25  userid_250  userid_251  userid_252  userid_253  userid_254  userid_255  userid_256  userid_257  userid_258  userid_259  userid_26  userid_260  userid_261  userid_262  userid_263  userid_264  userid_265  userid_266  userid_267  userid_268  userid_269  userid_27  userid_270  userid_271  userid_272  userid_273  userid_274  userid_275  userid_276  userid_277  userid_278  userid_279  userid_28  userid_280  userid_281  userid_282  userid_283  userid_284  userid_285  userid_286  userid_287  userid_288  userid_289  userid_29  userid_290  userid_291  userid_292  userid_293  userid_294  userid_295  userid_296  userid_297  userid_298  userid_299  userid_3  userid_30  userid_300  userid_301  userid_302  userid_303  userid_304  userid_305  userid_306  userid_307  userid_308  userid_309  userid_31  userid_310  userid_311  userid_312  userid_313  userid_314  userid_315  userid_316  userid_317  userid_318  userid_319  userid_32  userid_320  userid_321  userid_322  userid_323  userid_324  userid_325  userid_326  userid_327  userid_328  userid_329  userid_33  userid_330  userid_331  userid_332  userid_333  userid_334  userid_335  userid_336  userid_337  userid_338  userid_339  userid_34  userid_340  userid_341  userid_342  userid_343  userid_344  userid_345  userid_346  userid_347  userid_348  userid_349  userid_35  userid_350  userid_351  userid_352  userid_353  userid_354  userid_355  userid_356  userid_357  userid_358  userid_359  userid_36  userid_360  userid_361  userid_362  userid_363  userid_364  userid_365  userid_366  userid_367  userid_368  userid_369  userid_37  userid_370  userid_371  userid_372  userid_373  userid_374  userid_375  userid_376  userid_377  userid_378  userid_379  userid_38  userid_380  userid_381  userid_382  userid_383  userid_384  userid_385  userid_386  userid_387  userid_388  userid_389  userid_39  userid_390  userid_391  userid_392  userid_393  userid_394  userid_395  userid_396  userid_397  userid_398  userid_399  userid_4  userid_40  userid_400  userid_401  userid_402  userid_403  userid_404  userid_405  userid_406  userid_407  userid_408  userid_409  userid_41  userid_410  userid_411  userid_412  userid_413  userid_414  userid_415  userid_416  userid_417  userid_418  userid_419  userid_42  userid_420  userid_421  userid_422  userid_423  userid_424  userid_425  userid_426  userid_427  userid_428  userid_429  userid_43  userid_430  userid_431  userid_432  userid_433  userid_434  userid_435  userid_436  userid_437  userid_438  userid_439  userid_44  userid_440  userid_441  userid_442  userid_443  userid_444  userid_445  userid_446  userid_447  userid_448  userid_449  userid_45  userid_450  userid_451  userid_452  userid_453  userid_454  userid_455  userid_456  userid_457  userid_458  userid_459  userid_46  userid_460  userid_461  userid_462  userid_463  userid_464  userid_465  userid_466  userid_467  userid_468  userid_469  userid_47  userid_470  userid_471  userid_472  userid_473  userid_474  userid_475  userid_476  userid_477  userid_478  userid_479  userid_48  userid_480  userid_481  userid_482  userid_483  userid_484  userid_485  userid_486  userid_487  userid_488  userid_489  userid_49  userid_490  userid_491  userid_492  userid_493  userid_494  userid_495  userid_496  userid_497  userid_498  userid_499  userid_5  userid_50  userid_500  userid_501  userid_502  userid_503  userid_504  userid_505  userid_506  userid_507  userid_508  userid_509  userid_51  userid_510  userid_511  userid_512  userid_513  userid_514  userid_515  userid_516  userid_517  userid_518  userid_519  userid_52  userid_520  userid_521  userid_522  userid_523  userid_524  userid_525  userid_526  userid_527  userid_528  userid_529  userid_53  userid_530  userid_531  userid_532  userid_533  userid_534  userid_535  userid_536  userid_537  userid_538  userid_539  userid_54  userid_540  userid_541  userid_542  userid_543  userid_544  userid_545  userid_546  userid_547  userid_548  userid_549  userid_55  userid_550  userid_551  userid_552  userid_553  userid_56  userid_57  userid_58  userid_59  userid_6  userid_60  userid_61  userid_62  userid_63  userid_64  userid_65  userid_66  userid_67  userid_68  userid_69  userid_7  userid_70  userid_71  userid_72  userid_73  userid_74  userid_75  userid_76  userid_77  userid_78  userid_79  userid_8  userid_80  userid_81  userid_82  userid_83  userid_84  userid_85  userid_86  userid_87  userid_88  userid_89  userid_9  userid_90  userid_91  userid_92  userid_93  userid_94  userid_95  userid_96  userid_97  userid_98  userid_99\n",
      "uni_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
      "uni_id_0          NaN       NaN       3.94         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        3.89         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.92       3.73         NaN         NaN         NaN        2.67         NaN         NaN         NaN         NaN         NaN         NaN       4.29         NaN         3.2         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        3.28         NaN         NaN         NaN         NaN        3.24        NaN         NaN        4.06         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       4.74         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        4.22         NaN       NaN        NaN         NaN         NaN        3.85         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        4.01         NaN         NaN         NaN         NaN         NaN         NaN        2.96         NaN        3.58       3.85         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       2.98         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        2.32         NaN         NaN         NaN         NaN         NaN        3.26         NaN        NaN         NaN         4.3         NaN        3.01         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         3.6         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.68         NaN         NaN         NaN        2.84         NaN         NaN       NaN       3.91         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.85         NaN         NaN        NaN        3.00         NaN         NaN         NaN         NaN         NaN         NaN         3.6        2.88        2.86        NaN         NaN         NaN         NaN         NaN         NaN        3.43        3.61         NaN         NaN         NaN        NaN        3.04        3.33         NaN        3.09         NaN         NaN        4.06         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.25         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.75         NaN         NaN       4.44         NaN         NaN         NaN         NaN         NaN         NaN        3.71         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.93         NaN        3.36         NaN         NaN       3.72         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.56         NaN         NaN        3.91         NaN       NaN       3.74         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        3.73         NaN         NaN         NaN         NaN        3.91         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        2.71         NaN         NaN         NaN         NaN         3.4         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       NaN       4.09         NaN         NaN         NaN         NaN        3.58         NaN         NaN         NaN         NaN         NaN        NaN        4.29         NaN         NaN        4.00         NaN        3.38         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        3.46         NaN        3.99         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        3.89         NaN         NaN         NaN         NaN         NaN       3.41         NaN         NaN         NaN         NaN         NaN        3.32         NaN         NaN         NaN         NaN       3.19         NaN         NaN         NaN         NaN        NaN        NaN        NaN        NaN       NaN       4.95        NaN       3.88       4.41        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN       3.59        NaN        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN       4.31\n",
      "uni_id_1          NaN       NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.74         NaN         NaN         NaN        NaN         NaN         NaN         NaN        2.77         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.98         NaN         NaN         NaN        NaN         NaN         NaN        2.53         NaN         NaN         NaN         NaN         NaN         NaN         3.2        NaN         NaN         NaN         NaN         NaN         3.4         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.27         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.41         NaN         NaN        3.01         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.27         NaN         NaN         NaN         NaN        3.02         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       NaN       2.31         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        3.81         NaN         NaN        3.97         NaN         NaN        2.59         NaN        NaN        2.63        3.20         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        2.96         NaN         NaN         NaN         NaN         NaN         NaN        2.93         NaN        NaN         NaN         NaN         NaN        2.47         NaN        2.51         NaN         2.9         NaN        3.34        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.67        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.61        NaN         NaN         NaN         NaN         NaN         NaN        2.17         NaN         NaN         NaN         NaN        NaN        2.40         NaN         NaN         NaN         NaN         NaN         NaN        3.04         NaN         NaN       NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.78         NaN         NaN        3.16        NaN         NaN         NaN         NaN         NaN        3.51         NaN        3.22         NaN         NaN         NaN        NaN         NaN         NaN         NaN        4.54         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.24         NaN        2.62         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        2.89         NaN        2.37        3.64         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.35         NaN         NaN         NaN         NaN         NaN         NaN        NaN        1.44         NaN         NaN        2.66         NaN         NaN         NaN         NaN         NaN         NaN       NaN       2.56        3.25         NaN         NaN        2.59         NaN         NaN         NaN         NaN        2.56         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        1.82         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       2.08         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        3.87         NaN         NaN         NaN        2.41        2.25         NaN        2.55         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        2.41         NaN         NaN         NaN         NaN       3.39         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.08       NaN        NaN         NaN         NaN        3.36         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.03        2.72         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.41         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.18         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN       3.24        NaN        NaN        NaN        NaN        NaN       NaN        NaN       4.64        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN       3.42        NaN       3.81       3.86        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN\n",
      "uni_id_10         NaN       NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.41         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        3.05         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.40         NaN         NaN         NaN         NaN        3.53         NaN        NaN         NaN         NaN         NaN         NaN         3.1         NaN        3.42         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        3.19         NaN         NaN        3.96         NaN         NaN        NaN         NaN         NaN         NaN        2.95        2.89         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        3.28         NaN         NaN         NaN        2.42         NaN         NaN         NaN         NaN         NaN        NaN        3.46         NaN         NaN        3.89         NaN         NaN         NaN         NaN         NaN         NaN      3.79        NaN         NaN         NaN        3.25         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        3.26         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        3.74         NaN         NaN         NaN        1.95         NaN        NaN         NaN        3.55         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        3.13        3.97         NaN         NaN        2.72        3.39         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        3.21         NaN         NaN        3.10         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        2.72         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       3.64         NaN        4.12         NaN         NaN         NaN         NaN        2.94         NaN         NaN         NaN       NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       4.23         NaN         NaN        3.12         NaN         NaN         2.9         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.51        NaN         NaN        3.75         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        2.95        2.13         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        3.82        1.76         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        2.92        3.22         NaN        2.16         NaN         NaN         NaN         NaN         NaN       NaN       4.31         NaN         NaN         NaN         NaN        4.27        3.48         NaN         NaN         NaN         NaN       2.77         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        2.75         NaN         NaN         NaN         NaN        3.07         4.0         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        3.85         NaN         3.5         NaN         NaN         NaN        NaN         NaN         NaN        2.84         NaN         NaN         NaN         NaN         NaN        1.97         NaN       3.32         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.05         NaN         NaN        3.18         NaN         NaN         NaN       3.35         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       NaN        NaN         NaN         NaN        3.45         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        2.52         NaN        3.38         NaN         NaN        3.13         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.57         NaN         NaN         NaN         NaN        NaN        3.21         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.01         NaN        NaN         NaN        3.36         NaN         NaN        NaN        NaN        NaN        NaN       2.3        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN       4.03        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN\n",
      "uni_id_100        NaN       NaN       4.43         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.70        3.92         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.73         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        3.21         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.73        3.47         NaN         NaN        3.04        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        4.01         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        3.28         NaN        3.67         NaN         NaN         NaN         NaN         NaN         NaN         NaN       NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.38        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        3.42         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.99        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        2.15         NaN         NaN        3.06        3.49         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       3.08         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        2.37         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.13         NaN         NaN         NaN         NaN         NaN         NaN       NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.08        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        2.73         NaN         NaN         NaN        3.39         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN        3.72        3.86        2.73         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.31         NaN         NaN         NaN        3.29         NaN         NaN        NaN         NaN         NaN        3.75         NaN         NaN        3.20         NaN        3.96         NaN         NaN        NaN         NaN         NaN         NaN        2.19        4.26         NaN         NaN         NaN         NaN        3.11       3.49         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.75        2.04         NaN       NaN       3.81         NaN         NaN         NaN        3.65         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        2.97        4.49         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.46         NaN         NaN         NaN        NaN        3.13         NaN         NaN         NaN         NaN         NaN         NaN         NaN        4.02         NaN        NaN         NaN         NaN         NaN         NaN         NaN        4.37         NaN         NaN         NaN         NaN        NaN         NaN        2.76         NaN         2.0        2.35         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.47         NaN         NaN         NaN      3.73        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN        2.66         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.78         NaN       3.47         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        2.94         NaN         NaN        4.05         NaN        NaN         NaN        2.93         NaN         NaN        NaN       3.97        NaN        NaN       NaN       3.85        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN       3.74        NaN        NaN       3.09        NaN        NaN       NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN      4.05        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN\n",
      "uni_id_1000      3.79       NaN        NaN         2.8         NaN         NaN         NaN        3.29         NaN         NaN         NaN         NaN         NaN        NaN         NaN        3.65         NaN         NaN         NaN         NaN         NaN        3.56         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        3.83         NaN         NaN         NaN        2.95         NaN         NaN         NaN         NaN         NaN        NaN        4.19         NaN         NaN        4.41         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.83        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN        3.60         NaN         NaN         NaN         NaN         NaN        3.03       3.25         NaN        4.23         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.66         NaN         NaN       NaN        NaN        3.12         NaN         NaN         NaN         NaN         NaN         4.5         NaN         NaN         NaN        NaN         NaN         NaN         NaN        2.49         NaN         NaN        2.43         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.59         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.34         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.55         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.49         NaN         NaN         NaN        NaN        3.41         NaN         NaN         NaN        4.73         NaN         NaN         NaN         NaN         NaN       NaN        NaN         NaN         NaN         NaN        2.64         NaN         NaN         NaN         NaN         NaN         NaN        NaN        3.01         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        4.17         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.78         NaN         NaN         NaN         NaN        NaN         NaN        3.96         NaN         NaN         NaN         NaN         NaN         NaN        3.34         NaN        NaN         NaN         NaN         NaN         NaN        2.92         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN        3.04        3.23         NaN         NaN         NaN        NaN         NaN         NaN         NaN        4.16         NaN         NaN         NaN        4.35         NaN         NaN       3.67         NaN         NaN         NaN         NaN         NaN        3.77         NaN         NaN        3.16        2.93       NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.71       3.87         3.1         NaN         NaN         NaN         NaN         NaN         NaN        3.51         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN       4.95         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN        3.62         NaN         NaN        4.58         NaN         NaN         NaN         NaN         NaN         NaN       3.09         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.50         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.03         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        2.68         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.69        NaN         NaN         NaN        3.75        4.05         NaN         NaN         NaN         NaN        2.21         NaN       NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        3.92        NaN         NaN        4.31        4.16         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN         4.3         NaN         NaN         NaN         NaN         NaN         NaN         NaN         NaN        NaN         NaN        3.13         NaN        4.06         NaN         NaN         NaN         NaN         NaN         NaN       3.90        3.28         NaN         NaN        3.32        NaN       4.51        NaN        NaN       NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN        NaN       NaN        NaN       3.59        NaN        NaN        NaN       3.71       3.18        NaN        NaN        NaN      4.39       3.65        NaN        NaN        4.7        NaN        NaN        3.8        NaN        NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "melted_df_ratings = data_sparse_user_ratings.reset_index().melt(id_vars=['index'], var_name='user_id', value_name='ratings')\n",
    "melted_df_ratings.rename(columns={'index': 'uni_id'}, inplace=True)\n",
    "melted_df_ratings.to_csv('../user_item_df.csv', index=False)\n",
    "\n",
    "# calculate density & sparsity\n",
    "tmp = melted_df_ratings.count()\n",
    "actual_ratings = tmp['ratings']\n",
    "total_possible_entries = tmp['user_id']\n",
    "sparsity = 1 - (actual_ratings / total_possible_entries)\n",
    "density = actual_ratings / total_possible_entries\n",
    "print(f\"Sparsity: {sparsity:.4f}\")\n",
    "print(f\"Density: {density:.4f}\")\n",
    "\n",
    "ratings_matrix = melted_df_ratings.pivot(index='uni_id', columns='user_id', values='ratings')\n",
    "# print(ratings_matrix)\n",
    "print('User-Item Matrix:')\n",
    "print(ratings_matrix.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise is a Python scikit for building and analyzing recommender systems that deal with explicit rating data.\n",
    "# $ conda install -c conda-forge scikit-surprise\n",
    "from surprise import KNNBasic\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms.matrix_factorization import SVD, NMF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../user_item_df.csv').dropna()\n",
    "\n",
    "reader = Reader() #default is already 1-5\n",
    "dataset = Dataset.load_from_df(data[['user_id','uni_id','ratings']], reader) #It must have three columns, corresponding to the user (raw) ids, the item (raw) ids, and the ratings, in this order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uni_id     680527\n",
       "user_id    680527\n",
       "ratings    680527\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-based CF using kNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.5552  0.5558  0.5541  0.5550  0.0007  \n",
      "MAE (testset)     0.4431  0.4439  0.4425  0.4432  0.0006  \n",
      "Fit time          1.73    1.78    1.70    1.73    0.03    \n",
      "Test time         15.83   13.98   13.73   14.51   0.93    \n"
     ]
    }
   ],
   "source": [
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity\n",
    "    'user_based': True,  # User-based collaborative filtering\n",
    "    'min_support': 3,   # Minimum number of common items for similarity\n",
    "    'shrinkage': 100    # Shrinkage parameter in case of sparse data\n",
    "}\n",
    "\n",
    "# Define the algorithm\n",
    "user_cf = KNNBasic(k=20, min_k=1,sim_options=sim_options,verbose=True)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "user_cf_cv_results = cross_validate(user_cf, dataset, measures=['RMSE', 'MAE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.5737  0.5737  0.5733  0.5735  0.0002  \n",
      "MAE (testset)     0.4586  0.4574  0.4574  0.4578  0.0006  \n",
      "Fit time          80.94   64.41   85.29   76.88   9.00    \n",
      "Test time         132.41  148.56  139.00  139.99  6.63    \n"
     ]
    }
   ],
   "source": [
    "# Define Item-Based CF algorithm\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity\n",
    "    'user_based': False,  # Item-based collaborative filtering\n",
    "    'min_support': 5,   # Minimum number of common items for similarity\n",
    "    'shrinkage': 100    # Shrinkage parameter in case of sparse data\n",
    "}\n",
    "item_cf = KNNBasic(k=20, min_k=1,sim_options=sim_options, verbose=True)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "item_cf_cv_results = cross_validate(item_cf, dataset, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Vector Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "Evaluating RMSE, MAE of algorithm SVD on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.5337  0.5325  0.5324  0.5328  0.0006  \n",
      "MAE (testset)     0.4261  0.4252  0.4251  0.4255  0.0005  \n",
      "Fit time          2.48    2.81    2.67    2.65    0.14    \n",
      "Test time         1.91    1.41    1.44    1.59    0.23    \n"
     ]
    }
   ],
   "source": [
    "# Define SVD algorithm\n",
    "svd_algo = SVD(n_factors=10, n_epochs=20,verbose=True)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "svd_cv_results = cross_validate(svd_algo, dataset, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm NMF on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    0.6567  0.6542  0.6564  0.6558  0.0011  \n",
      "MAE (testset)     0.5288  0.5264  0.5286  0.5279  0.0011  \n",
      "Fit time          3.29    3.47    3.24    3.33    0.10    \n",
      "Test time         1.69    1.32    1.31    1.44    0.18    \n"
     ]
    }
   ],
   "source": [
    "# Define NMF algorithm\n",
    "nmf_algo = NMF(n_factors=10, n_epochs=20,biased=False)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "nmf_cv_results = cross_validate(nmf_algo, dataset, measures=['RMSE', 'MAE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Based CF - RMSE: 0.5550 (± 0.0007), MAE: 0.4432 (± 0.0006)\n",
      "Item-Based CF - RMSE: 0.5735 (± 0.0002), MAE: 0.4578 (± 0.0006)\n",
      "SVD - RMSE: 0.5328 (± 0.0006), MAE: 0.4255 (± 0.0005)\n",
      "NMF - RMSE: 0.6558 (± 0.0011), MAE: 0.5279 (± 0.0011)\n"
     ]
    }
   ],
   "source": [
    "def print_cv_results(algo_name, cv_results):\n",
    "    mean_rmse = cv_results['test_rmse'].mean()\n",
    "    mean_mae = cv_results['test_mae'].mean()\n",
    "    std_rmse = cv_results['test_rmse'].std()\n",
    "    std_mae = cv_results['test_mae'].std()\n",
    "    print(f\"{algo_name} - RMSE: {mean_rmse:.4f} (± {std_rmse:.4f}), MAE: {mean_mae:.4f} (± {std_mae:.4f})\")\n",
    "\n",
    "print_cv_results(\"User-Based CF\", user_cf_cv_results)\n",
    "print_cv_results(\"Item-Based CF\", item_cf_cv_results)\n",
    "print_cv_results(\"SVD\", svd_cv_results)\n",
    "print_cv_results(\"NMF\", nmf_cv_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
