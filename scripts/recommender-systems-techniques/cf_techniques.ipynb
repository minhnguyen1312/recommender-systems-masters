{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Data Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate real-world data where users rate only a subset of items, have each user rate a random N selection of programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = pd.read_csv('../university_user_ratings.csv').set_index('Unnamed: 0')\n",
    "user_ratings.index.names = ['index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust sparsity and density of User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_dataset(df_ratings_matrix, density_level):\n",
    "    \"\"\"\n",
    "    Create a sparse dataset by selecting a subset of ratings.\n",
    "\n",
    "    Parameters:\n",
    "    - ratings_df: DataFrame matrix containing the full ratings \n",
    "    - sparsity_level: Percentage of ratings to keep (e.g., 0.1 for 10%)\n",
    "\n",
    "    Returns:\n",
    "    - Sparse ratings DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    ### Calculate number of ratings, each user make (sparsity percent * total university)\n",
    "    rating_per_user = density_level * df_ratings_matrix.shape[0]\n",
    "    \n",
    "    sparse_ratings_df = pd.DataFrame().reindex_like(df_ratings_matrix)\n",
    "    for user_index in range(df_ratings_matrix.shape[1]):\n",
    "        # Each user randomly rate 0.9 - 1.1 of ratings avg\n",
    "        rating_per_user_threshold = random.randint(round(0.9 * rating_per_user), round(1.1 * rating_per_user))\n",
    "        uni_id_to_rate = random.sample(list(range(df_ratings_matrix.shape[0])), k=rating_per_user_threshold) \n",
    "        for uni_id in uni_id_to_rate:\n",
    "            sparse_ratings_df.loc[f'uni_id_{uni_id}', f'userid_{user_index}'] \\\n",
    "             = df_ratings_matrix.loc[f'uni_id_{uni_id}', f'userid_{user_index}']\n",
    "    return sparse_ratings_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity: 0.99\n",
      "Density: 0.01\n"
     ]
    }
   ],
   "source": [
    "sparsity_level = float(input(\"Choose sparsity level (0-1): \"))\n",
    "\n",
    "\n",
    "sparse_ratings_df = create_sparse_dataset (user_ratings, 1-sparsity_level)\n",
    "print(f\"Sparsity: {sparsity_level:.2f}\")\n",
    "print(f\"Density: {1-sparsity_level:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surprise is a Python scikit for building and analyzing recommender systems that deal with explicit rating data.\n",
    "# $ conda install -c conda-forge scikit-surprise\n",
    "from surprise import Dataset, Reader, KNNBasic, SVD, NMF, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5400000000000045\n"
     ]
    }
   ],
   "source": [
    "avg_ratings_per_user = (1- sparsity_level) * user_ratings.shape[0]\n",
    "avg_ratings_per_uni = (1- sparsity_level) * user_ratings.shape[1]\n",
    "print(avg_ratings_per_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build DataFrame for Train set\n",
    "melted_sparse_ratings = sparse_ratings_df.reset_index().melt(id_vars=['index'], var_name='user_id', value_name='ratings')\n",
    "melted_sparse_ratings.rename(columns={'index': 'uni_id'}, inplace=True)\n",
    "train_df = melted_sparse_ratings.dropna(inplace=False)\n",
    "\n",
    "# build DataFrame for Test set\n",
    "melted_user_ratings = user_ratings.reset_index().melt(id_vars=['index'], var_name='user_id', value_name='ratings')\n",
    "melted_user_ratings.rename(columns={'index': 'uni_id'}, inplace=True)\n",
    "test_df = melted_user_ratings.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader() #default is already 1-5\n",
    "\n",
    "sparse_rating_dataset = Dataset.load_from_df(train_df[['user_id', 'uni_id', 'ratings']], reader)\n",
    "trainset = sparse_rating_dataset.build_full_trainset()\n",
    "\n",
    "full_rating_dataset = Dataset.load_from_df(test_df[['user_id', 'uni_id', 'ratings']], reader)\n",
    "testset = full_rating_dataset.build_full_trainset().build_testset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Vector Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "RMSE: 0.2988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29883527415045935"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define SVD algorithm\n",
    "svd_algo = SVD(n_factors=15, n_epochs=20,verbose=True)\n",
    "\n",
    "svd_algo.fit(trainset)\n",
    "svd_predictions = svd_algo.test(testset)\n",
    "accuracy.rmse(svd_predictions, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.3312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.331230985047388"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define NMF algorithm\n",
    "nmf_algo = NMF(n_factors=15, n_epochs=20,biased=False,verbose=True)\n",
    "\n",
    "nmf_algo.fit(trainset)\n",
    "nmf_predictions = nmf_algo.test(testset)\n",
    "accuracy.rmse(nmf_predictions, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-based CF using kNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPredictionImpossible\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ngpbm\\anaconda3\\envs\\myenv\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:102\u001b[0m, in \u001b[0;36mAlgoBase.predict\u001b[1;34m(self, uid, iid, r_ui, clip, verbose)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miuid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miiid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# If the details dict was also returned\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ngpbm\\anaconda3\\envs\\myenv\\lib\\site-packages\\surprise\\prediction_algorithms\\knns.py:121\u001b[0m, in \u001b[0;36mKNNBasic.estimate\u001b[1;34m(self, u, i)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m actual_k \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_k:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PredictionImpossible(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough neighbors.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    123\u001b[0m est \u001b[38;5;241m=\u001b[39m sum_ratings \u001b[38;5;241m/\u001b[39m sum_sim\n",
      "\u001b[1;31mPredictionImpossible\u001b[0m: Not enough neighbors.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m user_cf_algo \u001b[38;5;241m=\u001b[39m KNNBasic(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, min_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,sim_options\u001b[38;5;241m=\u001b[39msim_options,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m user_cf_algo\u001b[38;5;241m.\u001b[39mfit(trainset)\n\u001b[1;32m---> 11\u001b[0m ub_predictions \u001b[38;5;241m=\u001b[39m \u001b[43muser_cf_algo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m accuracy\u001b[38;5;241m.\u001b[39mrmse(ub_predictions, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ngpbm\\anaconda3\\envs\\myenv\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:160\u001b[0m, in \u001b[0;36mAlgoBase.test\u001b[1;34m(self, testset, verbose)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03min the given testset.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    that contains all the estimated ratings.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# The ratings are translated back to their original scale.\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(uid, iid, r_ui_trans, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r_ui_trans) \u001b[38;5;129;01min\u001b[39;00m testset\n\u001b[0;32m    163\u001b[0m ]\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\ngpbm\\anaconda3\\envs\\myenv\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:161\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test the algorithm on given testset, i.e. estimate all the ratings\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03min the given testset.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    that contains all the estimated ratings.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# The ratings are translated back to their original scale.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_ui_trans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r_ui_trans) \u001b[38;5;129;01min\u001b[39;00m testset\n\u001b[0;32m    163\u001b[0m ]\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\ngpbm\\anaconda3\\envs\\myenv\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:111\u001b[0m, in \u001b[0;36mAlgoBase.predict\u001b[1;34m(self, uid, iid, r_ui, clip, verbose)\u001b[0m\n\u001b[0;32m    108\u001b[0m     details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas_impossible\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PredictionImpossible \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 111\u001b[0m     est \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas_impossible\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     details[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreason\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n",
      "File \u001b[1;32mc:\\Users\\ngpbm\\anaconda3\\envs\\myenv\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:128\u001b[0m, in \u001b[0;36mAlgoBase.default_prediction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28mprint\u001b[39m(pred)\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_prediction\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Used when the ``PredictionImpossible`` exception is raised during a\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    call to :meth:`predict()\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    <surprise.prediction_algorithms.algo_base.AlgoBase.predict>`. By\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m        (float): The mean of all ratings in the trainset.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset\u001b[38;5;241m.\u001b[39mglobal_mean\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity\n",
    "    'user_based': True,  # User-based collaborative filtering\n",
    "    'min_support': 5,   # Minimum number of common items for similarity\n",
    "}\n",
    "\n",
    "# Define the algorithm\n",
    "user_cf_algo = KNNBasic(k=20, min_k=1,sim_options=sim_options,verbose=True)\n",
    "\n",
    "user_cf_algo.fit(trainset)\n",
    "ub_predictions = user_cf_algo.test(testset)\n",
    "accuracy.rmse(ub_predictions, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item-based CF using kNN Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Item-Based CF algorithm\n",
    "sim_options = {\n",
    "    'name': 'cosine',  # Use cosine similarity\n",
    "    'user_based': False,  # Item-based collaborative filtering\n",
    "    'min_support': 5,   # Minimum number of common items for similarity\n",
    "}\n",
    "item_cf_algo = KNNBasic(k=20, min_k=1,sim_options=sim_options, verbose=True)\n",
    "\n",
    "item_cf_algo.fit(trainset)\n",
    "ib_predictions = item_cf_algo.test(testset)\n",
    "accuracy.rmse(ib_predictions, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"User-Based CF\")\n",
    "accuracy.rmse(ub_predictions, verbose=True)\n",
    "print(\"Item-Based CF\")\n",
    "accuracy.rmse(ib_predictions, verbose=True)\n",
    "print(\"SVD\")\n",
    "accuracy.rmse(svd_predictions, verbose=True)\n",
    "print(\"NMF\")\n",
    "accuracy.rmse(nmf_predictions, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
